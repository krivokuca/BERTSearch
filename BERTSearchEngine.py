'''
Article BERT Search Engine.
Version 0.05
=======================
The current version of the search engine uses a vectorized inverted index. Through BERT,
we index the articles such that each article contains a list of token tensors that appear in that
article. Then, we do the same to the query, finding tensors that are similar to the query and
returning those. By using BERT, it allows us to contextually group the 

DOCUMENT INDEXING
=================
1 - Each documents body is parsed and split into sentences
2 - The sentences are split into words, tokenized and stemmed
3 - Each token is then vectorized into a 3072-long vector
4 - The token vectors are then appended to the articles master tensor
5 - Sub-tensors in the master tensor that are the same are removed

QUERYING
=================
1 - The query is tokenized and converted into a list of tensors
2 - KNN is used to locate the closest tensors from the document corpus to the query tensors
3 - tf-idf is used to retrieve a list of articles that match the criteria the best
4 - The articles are returned and the query is complete
'''
# pylint: disable=E1102
import json
import torch
from nltk import tokenize
from transformers import pipeline, BertTokenizer, BertModel
from ErrorLogger import ErrorLogger


class BERTSearchEngine():
    """
    This class provides all the functionality that is needed to both build the token
    index, process the tokens, extract and vectorize the tokens using BERT and to
    ultimately search the latent vector space generated by BERT for a keyword or for
    an entity/ticker value
    """

    def __init__(self, application_path):
        """
        Instantiates the configuration file and the redis cache connection

        Parameters:
            - application_path :: The path to which the models will be saved to
        """
        self.classifier = Classifier()
        self.error_logger = ErrorLogger()
        self.application_path = application_path

    def make_index(self, body):
        """
        This function produces an index of vectors given an articles_id and it's body.
        It returns an array of each of the articles tensors
        """
        tensors = []
        try:
            sentences = tokenize.sent_tokenize(body)
            for sentence in sentences:
                sentence_vector = self.classifier.extract(sentence)
                tensors.append(sentence_vector)
                del sentence_vector

            tensors = list(set(tensors))
            article_tensor = torch.stack(tensors)
            return article_tensor
        except Exception as e:
            print("Exception caught")

    def save_tensors(self, tensors):
        """
        Saves the tensors in the specified path. Takes a list of tensors and saves them
        """
        torch.save(tensors, self.application_path+'article_index.pt')


class Classifier():
    """
    The classifier tokenizes, stems and encodes words/sentences into bert sentence embeddings
    """

    def __init__(self):
        torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.model = BertModel.from_pretrained(
            "bert-base-uncased", output_hidden_states=True)
        self.model
        self.model.eval()

    def _preproc(self, text):
        """
        Pre-processes and tokenizes the text
        """
        text = "[CLS] {} [SEP]".format(text)
        tokens = self.tokenizer.tokenize(text)
        indexes = self.tokenizer.convert_tokens_to_ids(tokens)
        segment_ids = [1] * len(tokens)
        return [indexes], [segment_ids]

    def extract(self, text):
        """
        Extracts the hidden state from the BERT model and returns the vectorized
        embeddings of the sentence
        """
        # proprocess the text
        tokens, segments = self._preproc(text)
        if not tokens:
            return False
        tokens = torch.tensor(tokens)
        segments = torch.tensor(segments)
        with torch.no_grad():
            outputs = self.model(tokens, segments)
            # extract the hidden states
            hidden_states = outputs[2]
            output = torch.stack(hidden_states, dim=0)
            output = torch.squeeze(output, dim=1)
            output = output.permute(1, 0, 2)

            # next, we need to extract our word vector from the output
            # the output has shape [13 x 1 x n x 768], we must get the sentence
            # vector into the shape [n x 768] (n = number of tokens)
            word_vectors = []
            for tensor in output:
                catenate = torch.cat(
                    (tensor[-1], tensor[-2], tensor[-3], tensor[-4]), dim=0)
                word_vectors.append(catenate)
            word_mean = torch.mean(torch.stack(word_vectors))
            del output
            del outputs
            del tokens
            del segments
            return word_mean
